{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68512dd4",
   "metadata": {},
   "source": [
    "# Title "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b78e70",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "text text text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30baa9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "text text text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221142b",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "text text text\n",
    "\n",
    "Describe what DMPS is and what bins are etc, cant be sassumed that reader knows what it is. Describe Zeppelin. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259840ec",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "\n",
    "And make sure that functions are auto-updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40a6e4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import DMPS_functions as fu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime \n",
    "import seaborn as sns\n",
    "import scipy as sc\n",
    "import glob \n",
    "\n",
    "# Auto-update the functions. Useful if the functions have been altered. \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c23acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the path containg all DMPS data\n",
    "path = 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85aabd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\DMPS_and_CPC_2010_QA_QC@STP_HARMONIZED_PT20210207.dat',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\DMPS_and_CPC_2011_QA_QC@STP_HARMONIZED_PT20210207.dat',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\DMPS_and_CPC_2012_QA_QC@STP_HARMONIZED_PT20210207.dat',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\DMPS_and_CPC_2013_QA_QC@STP_HARMONIZED_PT20210207.dat',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\DMPS_and_CPC_2014_QA_QC@STP_HARMONIZED_PT20210207.dat',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\DMPS_and_CPC_2015_QA_QC@STP_HARMONIZED_PT20210207.dat',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\DMPS_and_CPC_2016_QA_QC@STP_HARMONIZED_PT20210207.dat',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\DMPS_and_CPC_2017_QA_QC@STP_HARMONIZED_PT20210207.dat',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\DMPS_and_CPC_2018_QA_QC@STP_HARMONIZED_PT20210207.dat',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\DMPS_and_CPC_2019_QA_QC@STP_HARMONIZED_PT20210207.dat',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\DMPS_and_CPC_2020_QA_QC@STP_HARMONIZED_PT20210207.dat',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\README_BEFORE_USING_THIS_DATA.docx',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\Zeppelin_dNdlogDp_@STP_QA_QCOk_2010_PT20210207.tiff',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\Zeppelin_dNdlogDp_@STP_QA_QCOk_2011_PT20210207.tiff',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\Zeppelin_dNdlogDp_@STP_QA_QCOk_2012_PT20210207.tiff',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\Zeppelin_dNdlogDp_@STP_QA_QCOk_2013_PT20210207.tiff',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\Zeppelin_dNdlogDp_@STP_QA_QCOk_2014_PT20210207.tiff',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\Zeppelin_dNdlogDp_@STP_QA_QCOk_2015_PT20210207.tiff',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\Zeppelin_dNdlogDp_@STP_QA_QCOk_2016_PT20210207.tiff',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\Zeppelin_dNdlogDp_@STP_QA_QCOk_2017_PT20210207.tiff',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\Zeppelin_dNdlogDp_@STP_QA_QCOk_2018_PT20210207.tiff',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\Zeppelin_dNdlogDp_@STP_QA_QCOk_2019_PT20210207.tiff',\n",
       " 'C:\\\\Users\\\\Lovisa\\\\Documents\\\\Courses II\\\\E_science_tools HT22\\\\Project\\\\DATA_ZEP_2010to2020\\\\Zeppelin_dNdlogDp_@STP_QA_QCOk_2020_PT20210207.tiff']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using glob to import the files in the specified path\n",
    "glob.glob(path+'\\\\*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3791db73",
   "metadata": {},
   "source": [
    "Loading each year of 30 min resolution DMPS data into a dataframe and create a list containing all dataframes.\n",
    "Column headings are added, and data flagged as invalid is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b4da706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Year', 'Month', 'Day', 'Hour', 'Minute', 'UFCPC', 'CPC3010', 'Ntot', 'unknown4', 'unknown5', 'unknown6', 'unknown7', 'unknown8', '5.0118723e-09', '5.6234133e-09', '6.3095734e-09', '7.0794578e-09', '7.9432823e-09', '8.9125094e-09', '1.0000000e-08', '1.1220185e-08', '1.2589254e-08', '1.4125375e-08', '1.5848932e-08', '1.7782794e-08', '1.9952623e-08', '2.2387211e-08', '2.5118864e-08', '2.8183829e-08', '3.1622777e-08', '3.5481339e-08', '3.9810717e-08', '4.4668359e-08', '5.0118723e-08', '5.6234133e-08', '6.3095734e-08', '7.0794578e-08', '7.9432823e-08', '8.9125094e-08', '1.0000000e-07', '1.1220185e-07', '1.2589254e-07', '1.4125375e-07', '1.5848932e-07', '1.7782794e-07', '1.9952623e-07', '2.2387211e-07', '2.5118864e-07', '2.8183829e-07', '3.1622777e-07', '3.5481339e-07', '3.9810717e-07', '4.4668359e-07', '5.0118723e-07', '5.6234133e-07', '6.3095734e-07', '7.0794578e-07', 'flag']\n",
      "C:\\Users\\Lovisa\\Documents\\Courses II\\E_science_tools HT22\\Project\\DATA_ZEP_2010to2020\\DMPS_and_CPC_2010_QA_QC@STP_HARMONIZED_PT20210207.dat\n",
      "Size without flags removed: 17520\n",
      "Size flags removed: 5088\n",
      "C:\\Users\\Lovisa\\Documents\\Courses II\\E_science_tools HT22\\Project\\DATA_ZEP_2010to2020\\DMPS_and_CPC_2011_QA_QC@STP_HARMONIZED_PT20210207.dat\n",
      "Size without flags removed: 17520\n",
      "Size flags removed: 9601\n",
      "C:\\Users\\Lovisa\\Documents\\Courses II\\E_science_tools HT22\\Project\\DATA_ZEP_2010to2020\\DMPS_and_CPC_2012_QA_QC@STP_HARMONIZED_PT20210207.dat\n",
      "Size without flags removed: 17568\n",
      "Size flags removed: 14887\n",
      "C:\\Users\\Lovisa\\Documents\\Courses II\\E_science_tools HT22\\Project\\DATA_ZEP_2010to2020\\DMPS_and_CPC_2013_QA_QC@STP_HARMONIZED_PT20210207.dat\n",
      "Size without flags removed: 17520\n",
      "Size flags removed: 15116\n",
      "C:\\Users\\Lovisa\\Documents\\Courses II\\E_science_tools HT22\\Project\\DATA_ZEP_2010to2020\\DMPS_and_CPC_2014_QA_QC@STP_HARMONIZED_PT20210207.dat\n",
      "Size without flags removed: 17520\n",
      "Size flags removed: 8492\n",
      "C:\\Users\\Lovisa\\Documents\\Courses II\\E_science_tools HT22\\Project\\DATA_ZEP_2010to2020\\DMPS_and_CPC_2015_QA_QC@STP_HARMONIZED_PT20210207.dat\n",
      "Size without flags removed: 17520\n",
      "Size flags removed: 7287\n",
      "C:\\Users\\Lovisa\\Documents\\Courses II\\E_science_tools HT22\\Project\\DATA_ZEP_2010to2020\\DMPS_and_CPC_2016_QA_QC@STP_HARMONIZED_PT20210207.dat\n",
      "Size without flags removed: 17568\n",
      "Size flags removed: 10646\n",
      "C:\\Users\\Lovisa\\Documents\\Courses II\\E_science_tools HT22\\Project\\DATA_ZEP_2010to2020\\DMPS_and_CPC_2017_QA_QC@STP_HARMONIZED_PT20210207.dat\n",
      "Size without flags removed: 17520\n",
      "Size flags removed: 13949\n",
      "C:\\Users\\Lovisa\\Documents\\Courses II\\E_science_tools HT22\\Project\\DATA_ZEP_2010to2020\\DMPS_and_CPC_2018_QA_QC@STP_HARMONIZED_PT20210207.dat\n",
      "Size without flags removed: 17520\n",
      "Size flags removed: 9611\n",
      "C:\\Users\\Lovisa\\Documents\\Courses II\\E_science_tools HT22\\Project\\DATA_ZEP_2010to2020\\DMPS_and_CPC_2019_QA_QC@STP_HARMONIZED_PT20210207.dat\n",
      "Size without flags removed: 17520\n",
      "Size flags removed: 8238\n",
      "C:\\Users\\Lovisa\\Documents\\Courses II\\E_science_tools HT22\\Project\\DATA_ZEP_2010to2020\\DMPS_and_CPC_2020_QA_QC@STP_HARMONIZED_PT20210207.dat\n",
      "Size without flags removed: 17568\n",
      "Size flags removed: 14153\n"
     ]
    }
   ],
   "source": [
    "DFs_DMPS = fu.load_and_append_DMPS(path, \n",
    "                             name_in_file='DMPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c4803",
   "metadata": {},
   "source": [
    "### Concatenate DMPS data \n",
    "From the list containg one datafram per year, one datframe containg data from all years is created. The columns headings for the bin midpoint diameters are given in the  are renamed to names that are easier to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b34ae6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the dataframes in list DFs_DMPS to one dataframe and drop unnessesary columns.\n",
    "df_DMPS = fu.concat_df_DMPS(DFs_DMPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffef984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list with the DMPS bin column names as strings \n",
    "bin_col_list = fu.get_bin_column_string_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34262c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename the column headings for midpoint diameters to numeric values with less decimals.\n",
    "df_DMPS = fu.renameDpColumns(df_DMPS, bin_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d55accc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of all the \"real\" non-rounded midpoint diameters as a list and as array.\n",
    "diameterList, diameters = fu.getFloatDiameterListAndArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d66e28",
   "metadata": {},
   "source": [
    "### Resampling \n",
    "The function ``resample`` was used to convert the 30 min DMPS data to dataframes containing the daily mean, median and 1 hour average. The python function ``resample`` fills the gaps in the data by adding rows for gap days/months/hours with NaN-values to get a continous dataset. Because of this, rows containing only NaN values are dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756ce38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily_2010_2020_mean   = df_DMPS.resample('D').mean()\n",
    "df_daily_2010_2020_mean   = df_daily_2010_2020_mean.dropna(how='all')\n",
    "\n",
    "df_daily_2010_2020_median = df_DMPS.resample('D').median()\n",
    "df_daily_2010_2020_median = df_daily_2010_2020_median.dropna(how='all') \n",
    "\n",
    "df_hourly_2010_2020_mean  = df_DMPS.resample('H').mean()\n",
    "df_hourly_2010_2020_mean  = df_hourly_2010_2020_mean.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e9ffc",
   "metadata": {},
   "source": [
    "Describe what the columns are in the data? Maybe this should be done at an earlier stage.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a115f9c",
   "metadata": {},
   "source": [
    "### Methods to identify NPF events\n",
    "Some different methods are tested to identify the NPF events: \n",
    "* By calculation of $\\frac{N_{D_p<10nm}}{N_{tot}}$ \n",
    "* By using K-means clustering\n",
    "* By calculating the difference between the UF-CPC and the CPC \n",
    "\n",
    "The first method requires that the total number concentration $N_{tot}$ is calculated for a given diameter range. For that the log-normal distribution which is provided by the DMPS have to be integrated. Linn Karlsson (e-Science course participant 2017) should be acknowledged for inspiration to the method used in this work (Linn's method invloved the use of dictionaries but here arrays are used instead).  \n",
    "\n",
    "The ``calcNtot`` calculates the particle concentration by integrating in a given bin midpoint diameter interval and adds a column in the dataframe containg the calculated values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72cf4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total particle number concentration\n",
    "df_daily_2010_2020_median_ntotCalc = fu.calcNtot(diameters, df_daily_2010_2020_median)\n",
    "df_DMPS_ntotCalc                   = fu.calcNtot(diameters, df_DMPS) # 30 min res\n",
    "df_hourly_2010_2020_mean_ntotCalc  = fu.calcNtot(diameters, df_hourly_2010_2020_mean) # 30 min res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diameter_list = list(diameters)\n",
    "# df_daily_2010_2020_median[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe1614",
   "metadata": {},
   "source": [
    "# STOP 22115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3502b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that boundries are correct\n",
    "\n",
    "# ys = [0,10]\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# for i in range(len(diameters)):\n",
    "#     ls = [diameters[i], diameters[i]]\n",
    "#     ax.plot(ls,ys,'r:')\n",
    "#     ax.set_xscale('log')\n",
    "    \n",
    "# for i in range(len(upperBoundaries)):\n",
    "#     ls2 = [upperBoundaries[i],upperBoundaries[i]]\n",
    "#     ax.plot(ls2,ys,'b')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91539473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls = [diameters[i], diameters[i]]\n",
    "# print(ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25a783",
   "metadata": {},
   "source": [
    "### N_tot from CPC vs calclated N_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1069de",
   "metadata": {},
   "outputs": [],
   "source": [
    "varx = df_hourly_2010_2020_mean_ntotCalc['NtotCalc'].values\n",
    "vary = df_hourly_2010_2020_mean_ntotCalc['Ntot'].values\n",
    "\n",
    "mask = ~np.isnan(varx) & ~np.isnan(vary)\n",
    "res = sc.stats.linregress(varx[mask], vary[mask])\n",
    "\n",
    "print(f\"R-squared: {res.rvalue**2:.6f}\")\n",
    "\n",
    "plt.plot(varx,\n",
    "         vary,\n",
    "         'o', label='original data')\n",
    "plt.plot(varx,\n",
    "         res.intercept + res.slope*varx,\n",
    "         'r-', label='fitted line')\n",
    "plt.legend()\n",
    "plt.show()    \n",
    "plt.ylabel('N_tot')\n",
    "plt.xlabel('N_tot_calc')\n",
    "print(res.intercept)\n",
    "print(res.slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056e75c",
   "metadata": {},
   "source": [
    "### Compare $N_{D_P <x}/N_{tot}$ and UFCPC/CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0087e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the area up to x nm with total area \n",
    "# Vary the threshold diameter x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f096c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold = 10\n",
    "\n",
    "bin_cols = get_bins(bin_col_list)\n",
    "#print(bin_cols)\n",
    "bin_cols = [float(x) for x in bin_cols]\n",
    "bin_cols = [x for x in bin_cols if x < threshold]\n",
    "#print(bin_cols)\n",
    "\n",
    "# df changes\n",
    "df_daily_2010_2020_median_copy = df_daily_2010_2020_median.copy(deep = True)\n",
    "df_tmp = calcNtot(diameters[:len(bin_cols)+1], df_daily_2010_2020_median_copy)\n",
    "\n",
    "# Plot Nx/Ntot\n",
    "NxNtot = df_tmp['NtotCalc'].values/df_tmp['Ntot'].values\n",
    "\n",
    "fig, axs = plt.subplots(4,figsize=(15, 15))\n",
    "\n",
    "axs[0].plot(df_tmp.index,NxNtot,'o')\n",
    "axs[0].set_xlabel('Datetime')\n",
    "axs[0].set_ylabel('$N_{D_P < x}/N_{tot}$')\n",
    "axs[0].set_title('N calculated with CalcNtot')\n",
    "\n",
    "df_tmp['NxNtot'] = df_tmp['NtotCalc']/df_tmp['Ntot']\n",
    "df_annual_cycle = df_tmp['NxNtot'].groupby(df_tmp.index.month).median()\n",
    "\n",
    "axs[1].plot(df_annual_cycle.index, df_annual_cycle.values)\n",
    "axs[1].set_xlabel('Month of the year')\n",
    "axs[1].set_ylabel('Median $N_{D_P < x}/N_{tot}$')\n",
    "axs[1].set_title('Annual cycle $N_{D_P < x}/N_{tot}$ ')\n",
    "\n",
    "df_tmp['ratio_CPCs'] = df_tmp['UFCPC']/df_tmp['CPC3010']\n",
    "df_annual_cycle_CPC = df_tmp['ratio_CPCs'].groupby(df_tmp.index.month).median()\n",
    "\n",
    "axs[2].plot(df_annual_cycle_CPC.index, df_annual_cycle_CPC.values)\n",
    "axs[2].set_xlabel('Month of the year')\n",
    "axs[2].set_ylabel('Median $N_{D_P < x}/N_{tot}$')\n",
    "axs[2].set_title('Annual cycle UFCPC/CPC')\n",
    "\n",
    "# Diffs are so small so ratio is bad, abs diff is better\n",
    "absDiffCPCs = np.absolute(df_tmp['UFCPC'].values - df_tmp['CPC3010'].values)\n",
    "df_tmp['AbsDiffCPCs'] = absDiffCPCs\n",
    "df_annual_cycle_abs = df_tmp['AbsDiffCPCs'].groupby(df_tmp.index.month).median()\n",
    "\n",
    "axs[3].plot(df_annual_cycle_abs.index, df_annual_cycle_abs.values)\n",
    "axs[3].set_xlabel('Month of the year')\n",
    "axs[3].set_ylabel('Abs(UFCPC-CPC)')\n",
    "axs[3].set_title('Annual cycle abs(UFCPC-CPC)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dda024",
   "metadata": {},
   "source": [
    "### Plot daily comparison of $N_{D_P <x}/N_{tot}$ and UFCPC/CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f094ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Nx/Ntot\n",
    "\n",
    "fig, axs = plt.subplots(3, figsize=(15, 15))\n",
    "\n",
    "axs[0].plot(df_tmp.index,df_tmp['NxNtot'].values,'o')\n",
    "axs[0].set_xlabel('Days',fontsize=20)\n",
    "axs[0].set_ylabel('Daily median $N_{D_P < x}/N_{tot}$',fontsize=20)\n",
    "axs[0].set_title('$N_{D_P < x}/N_{tot}$',fontsize=20)\n",
    "\n",
    "df_tmp['ratio_CPCs'] = df_tmp['UFCPC']/df_tmp['CPC3010']\n",
    "df_annual_cycle_CPC = df_tmp['ratio_CPCs'].groupby(df_tmp.index.month).median()\n",
    "\n",
    "axs[1].plot(df_tmp.index, df_tmp['ratio_CPCs'].values,'o')\n",
    "axs[1].set_xlabel('Days',fontsize=20)\n",
    "axs[1].set_ylabel('UFCPC/CPC',fontsize=20)\n",
    "axs[1].set_title('Cropped UFCPC/CPC',fontsize=20)\n",
    "axs[1].set_ylim([0, 5])\n",
    "\n",
    "axs[2].plot(df_tmp.index,absDiffCPCs,'o')\n",
    "axs[2].set_xlabel('Datetime')\n",
    "axs[2].set_ylabel('$abs(UF CPC - CPC)$')\n",
    "axs[2].set_title('Absolute difference between CPC:s')\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcdf9a2",
   "metadata": {},
   "source": [
    "## Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57acd909",
   "metadata": {},
   "source": [
    "#### Function to normalize the size distributions before clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_normalised_df(dataFrame, start_size_bin_col='5.012', end_size_bin_col='707.946'):\n",
    "    df = dataFrame.copy()\n",
    "    \n",
    "    n_vars = ['norm'+str(df.loc[:, start_size_bin_col:end_size_bin_col].columns.tolist()[i]) for i in range(0, df.loc[:, start_size_bin_col:end_size_bin_col].shape[1])]\n",
    "    print(n_vars)\n",
    "    #divide by maximum\n",
    "    df[n_vars] = df.loc[:, start_size_bin_col:end_size_bin_col].div(df.loc[:, start_size_bin_col:end_size_bin_col].max(axis=1), axis=0)\n",
    "    \n",
    "    df = df.loc[df.loc[:,start_size_bin_col:end_size_bin_col].dropna().index]\n",
    "    \n",
    "    Datetime_index = df.index    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    #print(\"Size of df after removing np.nan: \"+str(len(df)))\n",
    "    \n",
    "    start_size_normbin_col = 'norm'+str(start_size_bin_col)\n",
    "    end_size_normbin_col = 'norm'+str(end_size_bin_col)    \n",
    "    \n",
    "    df_norm = df.loc[:, start_size_normbin_col:end_size_normbin_col].copy()\n",
    "    df_norm.index = Datetime_index\n",
    "    df.index = Datetime_index\n",
    "    \n",
    "    print(\"Shape of df_norm: \"+str(df_norm.shape))\n",
    "    print(\"Shape of df: \"+str(df.shape))\n",
    "    return df, df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac6ea51",
   "metadata": {},
   "source": [
    "#### Function to perform clustering of normalized size distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050519e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(df_normarlised, n_clusters):\n",
    "    kmeans = KMeans(init=\"k-means++\", n_clusters=n_clusters).fit(df_normarlised) #Compute k-means clustering.\n",
    "    labels = kmeans.labels_\n",
    "    centres = kmeans.cluster_centers_\n",
    "    \n",
    "    # Predict the closest cluster each sample in X belongs to and add a column in the dataframe called clusters\n",
    "    df_normarlised['clusters'] = kmeans.predict(df_normarlised) \n",
    "    \n",
    "    df_normalized_copy = df_normarlised.copy()\n",
    "    print(df_normalized_copy['clusters'].unique())    \n",
    "    \n",
    "    df_normalized_copy['clusters'] = df_normalized_copy['clusters']+1\n",
    "    print(df_normalized_copy['clusters'].unique())\n",
    "    \n",
    "    dict_max_columns_unordered, dict_max_columns = produce_dicts_for_sorting(df_normalized_copy)\n",
    "    final_mapping = connect_dicts(dict_max_columns_unordered, dict_max_columns)\n",
    "    \n",
    "    df_normalized_copy['clusters'] = df_normalized_copy['clusters'].map(final_mapping)\n",
    "    \n",
    "    print(df_normalized_copy['clusters'].unique())\n",
    "    \n",
    "    return df_normalized_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7121ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_dicts_for_sorting(df):\n",
    "    df_ = df.groupby('clusters').mean()\n",
    "    max_col_index = df_.apply(lambda x: x.argmax(), axis=1)\n",
    "    dict_max_columns_unordered = dict(zip(df_.index, max_col_index))\n",
    "    max_col_index_sorted = sorted(max_col_index.values)\n",
    "    dict_max_columns = dict(zip(df_.index, max_col_index_sorted))\n",
    "    print(dict_max_columns)\n",
    "    return dict_max_columns_unordered, dict_max_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e845bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_max_columns_unordered, dict_max_columns = produce_dicts_for_sorting(df)\n",
    "# final_mapping = connect_dicts(dict_max_columns_unordered, dict_max_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_dicts(dict_max_columns_unordered, dict_max_columns):\n",
    "    final_mapping = {}\n",
    "    for k,v in dict_max_columns_unordered.items():\n",
    "        ordered_v = dict_max_columns[k]\n",
    "        ordered_v_list = list(dict_max_columns.values())\n",
    "        new_key = ordered_v_list.index(v)+1    \n",
    "        final_mapping[k] = new_key\n",
    "    print(\"the mapping from clustering to ordered clusters (by mode): \"+str(final_mapping))\n",
    "    return final_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92bcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_mapping = connect_dicts(dict_max_columns_unordered, dict_max_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ = df.groupby('clusters').mean()\n",
    "# print(df_)\n",
    "# df_['Max'] = df_.idxmax(axis=1)\n",
    "# dict_max_columns = dict(zip(df_.index, df_['Max'].values))\n",
    "# print(dict_max_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0eb257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ = df.groupby('clusters').mean()\n",
    "# print(df_)\n",
    "# max_values = df_.max(axis=1)\n",
    "# print(max_values)\n",
    "# for max_value in max_values:\n",
    "#     print(max_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c8a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfx = renameClusters(df_norm_clustered_1h_mean)\n",
    "\n",
    "#print(tmp = df[1,:]) norm5.623      0.130880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef54304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function to normalize size distributions \n",
    "\n",
    "df1, df_daily_2010_2020_median_norm = create_normalised_df(df_daily_2010_2020_median)\n",
    "df2, df_daily_2010_2020_mean_norm = create_normalised_df(df_daily_2010_2020_mean)\n",
    "df3, df_daily_2010_2020_1h_mean_norm = create_normalised_df(df_hourly_2010_2020_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_hourly_2010_2020_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2d5a2",
   "metadata": {},
   "source": [
    "#### Check that are normalized (only snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d74e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jj = np.arange(7,25,1)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# for i in jj:\n",
    "#     df_tmp_norm = df_daily_2010_2020_median_norm.iloc[i,:].to_frame() # Omit last column with -1 as that is flags\n",
    "    \n",
    "#     ax.plot(diameters*10**9, df_tmp_norm.values,'r')\n",
    "#     ax.set_xscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b7430",
   "metadata": {},
   "source": [
    "#### Cluster daily median and mean data and 1h hour mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596321b",
   "metadata": {},
   "source": [
    "##### Define the number of clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_clustered_median = perform_clustering(df_daily_2010_2020_median_norm, n_clusters)\n",
    "df_norm_clustered_mean = perform_clustering(df_daily_2010_2020_mean_norm, n_clusters)\n",
    "\n",
    "# 1 hour data needs special treatment:\n",
    "# Drop the 1st and two last bins before clustering\n",
    "df_hourly_norm_dropped = df_daily_2010_2020_1h_mean_norm.copy()\n",
    "df_hourly_norm_dropped = df_hourly_norm_dropped.drop \\\n",
    "                         (columns=['norm5.012','norm630.957','norm707.946'])\n",
    "\n",
    "df_norm_clustered_1h_mean = perform_clustering(df_hourly_norm_dropped, n_clusters)\n",
    "print(df_norm_clustered_1h_mean['clusters'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_hourly_norm_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1249325",
   "metadata": {},
   "source": [
    "#### Cluster 1h-mean data (not normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ac7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First drop the columns that should not be clustered\n",
    "df_hourly_2010_2020_mean_cleaned = df_hourly_2010_2020_mean.copy()\n",
    "print(df_hourly_2010_2020_mean.shape)\n",
    "df_hourly_2010_2020_mean_cleaned = df_hourly_2010_2020_mean_cleaned.drop \\\n",
    "                                   (columns=['UFCPC','5.012','630.957','707.946','CPC3010','Ntot','flag'])\n",
    "\n",
    "print(df_hourly_2010_2020_mean_cleaned.shape)\n",
    "\n",
    "def countNans(df):\n",
    "    no_of_rows, no_of_cols = df.shape\n",
    "    list_of_nans = []\n",
    "    for i in range(no_of_cols):\n",
    "        tmp = df.iloc[:,i].values\n",
    "        no_of_nans = tmp[np.isnan(tmp)]\n",
    "        list_of_nans.append( len(no_of_nans) )\n",
    "    return list_of_nans\n",
    "        \n",
    "list_of_nans = countNans(df_hourly_2010_2020_mean_cleaned)  \n",
    "#print(list_of_nans)\n",
    "# Conclusion: I drop the first size bin as it contains the majority of the nans\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform clustering of non-nromalized distributions NaNs must be dropped (done automatically in normalization)\n",
    "print('Shape before dropping Nans:',df_hourly_2010_2020_mean_cleaned.shape)\n",
    "\n",
    "df_hourly_2010_2020_mean_cleaned = df_hourly_2010_2020_mean_cleaned \\\n",
    "                                    .loc[df_hourly_2010_2020_mean_cleaned.loc[:,:].dropna().index]\n",
    "print('Shape after dropping Nans:',df_hourly_2010_2020_mean_cleaned.shape)\n",
    "\n",
    "df_clustered_1h_mean = perform_clustering(df_hourly_2010_2020_mean_cleaned, n_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd82dbbb",
   "metadata": {},
   "source": [
    "#### Cluster 1h-mean data (normalized distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e8d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at 1h mean clusters (from excluding the first bin)\n",
    "\n",
    "# I have dropped two last columns + first (bins that is)\n",
    "df_mean_1h_clusters = df_clustered_1h_mean.groupby('clusters').mean()\n",
    "df_std_1h_clusters  = df_clustered_1h_mean.groupby('clusters').std()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    ax.plot(diameters[1:-2]*10**9, df_mean_1h_clusters.iloc[i,:].values, '-')\n",
    "    \n",
    "    ax.fill_between(diameters[1:-2]*10**9, df_mean_1h_clusters.iloc[i,:].values+df_std_1h_clusters.iloc[i,:].values,                    \n",
    "                        df_mean_1h_clusters.iloc[i,:].values-df_std_1h_clusters.iloc[i,:].values, alpha=0.2)\n",
    "    \n",
    "    #ax.plot(diameters*10**9, df_median_clusters.iloc[i,:].values, 'k-')\n",
    "    #ax.set_xticks(bin_cols[::5])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    #ax.set_ylim(0,1.1)\n",
    "plt.xlabel('Dp [nm]')\n",
    "plt.title('1h mean clusters')\n",
    "plt.ylabel('Not norm concentration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_1h_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c2e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(diameters[1:-2]*10**9)\n",
    "print(df_mean_1h_clusters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hourly_2010_2020_mean.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159625d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check to see how median clusters look like---------------------------------------\n",
    "\n",
    "\n",
    "df_median_clusters = df_norm_clustered_median.groupby('clusters').median()\n",
    "df_quantile10_clusters = df_norm_clustered_median.groupby('clusters').quantile(.1)\n",
    "df_quantile90_clusters = df_norm_clustered_median.groupby('clusters').quantile(.9)\n",
    "\n",
    "df_mean_clusters = df_norm_clustered_mean.groupby('clusters').mean()\n",
    "df_std_clusters = df_norm_clustered_mean.groupby('clusters').std()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    ax.plot(diameters*10**9, df_median_clusters.iloc[i,:].values, '-')\n",
    "    ax.plot(diameters*10**9, df_mean_clusters.iloc[i,:].values, 'k-')\n",
    "    ax.fill_between(diameters*10**9, df_quantile90_clusters.iloc[i,:].values,\n",
    "                        df_quantile10_clusters.iloc[i,:].values, alpha=0.2)\n",
    "    #ax.set_xticks(bin_cols[::5])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(0,1.1)\n",
    "plt.xlabel('Dp [nm]')\n",
    "plt.title('Median daily clusters')\n",
    "plt.ylabel('Normalised concentration')\n",
    "plt.show()\n",
    "\n",
    "# Check to see how mean daily clusters look like---------------------------------------\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    ax.plot(diameters*10**9, df_mean_clusters.iloc[i,:].values, '-')\n",
    "    ax.fill_between(diameters*10**9, df_mean_clusters.iloc[i,:].values+df_std_clusters.iloc[i,:].values,                    \n",
    "                        df_mean_clusters.iloc[i,:].values-df_std_clusters.iloc[i,:].values, alpha=0.2)\n",
    "    ax.plot(diameters*10**9, df_median_clusters.iloc[i,:].values, 'k-')\n",
    "    #ax.set_xticks(bin_cols[::5])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(0,1.1)\n",
    "    \n",
    "    \n",
    "plt.xlabel('Dp [nm]')\n",
    "plt.title('Mean daily clusters')\n",
    "plt.ylabel('Normalised concentration')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895330d4",
   "metadata": {},
   "source": [
    "## Figures for presentation: 1h data Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57aaa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see how 1h mean clusters look like---------------------------------------\n",
    "# !!! Note that columns have been dropped !!!! -------------------------------------\n",
    "\n",
    "df_norm_clustered_1h_mean_median = df_norm_clustered_1h_mean.groupby('clusters').median()\n",
    "df_norm_clustered_1h_10q = df_norm_clustered_1h_mean.groupby('clusters').quantile(.1)\n",
    "df_norm_clustered_1h_90q = df_norm_clustered_1h_mean.groupby('clusters').quantile(.9)\n",
    "\n",
    "\n",
    "df_norm_clustered_1h_mean_mean = df_norm_clustered_1h_mean.groupby('clusters').mean()\n",
    "df_norm_clustered_1h_std = df_norm_clustered_1h_mean.groupby('clusters').std()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    ax.plot(diameters[1:-2]*10**9, df_norm_clustered_1h_mean_mean.iloc[i,:].values, '-')\n",
    "    ax.fill_between(diameters[1:-2]*10**9, \n",
    "                    df_norm_clustered_1h_mean_mean.iloc[i,:].values + df_norm_clustered_1h_std.iloc[i,:].values,                    \n",
    "                    df_norm_clustered_1h_mean_mean.iloc[i,:].values - df_norm_clustered_1h_std.iloc[i,:].values, alpha=0.2)\n",
    "    \n",
    "    # Plot the median to see similarity\n",
    "    ax.plot(diameters[1:-2]*10**9, df_norm_clustered_1h_mean_median.iloc[i,:].values, 'k-')\n",
    "    #ax.set_xticks(bin_cols[::5])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(0,1.1)\n",
    "    for spine in ['top', 'right']:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "        ax.spines[spine].set_linewidth(3)\n",
    "    \n",
    "  \n",
    "    \n",
    "plt.xlabel('Dp [nm]')\n",
    "plt.title('1h mean clusters (mean+ 1std)')\n",
    "plt.ylabel('Normalised concentration')\n",
    "plt.show()\n",
    "\n",
    "# Median 1h mean clusters----------------------------------------------\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "clusters = [1,2,3,4,5]\n",
    "\n",
    "for cluster in clusters:\n",
    "    #print(cluster)\n",
    "    df_cluster = df_norm_clustered_1h_mean_median[df_norm_clustered_1h_mean_median.index == cluster]\n",
    "    df_cluster_mean = df_norm_clustered_1h_mean_mean[df_norm_clustered_1h_mean_mean.index == cluster]\n",
    "    df_cluster90 = df_norm_clustered_1h_90q[df_norm_clustered_1h_90q.index == cluster]\n",
    "    df_cluster10 = df_norm_clustered_1h_10q[df_norm_clustered_1h_10q.index == cluster]\n",
    "    \n",
    "    ax.plot(diameters[1:-2]*10**9, df_cluster.iloc[0,:].values, '-', \n",
    "            label='cluster: '+str(cluster))\n",
    "    ax.fill_between(diameters[1:-2]*10**9, \n",
    "                    df_cluster90.iloc[0,:].values,\n",
    "                    df_cluster10.iloc[0,:].values,\n",
    "                    alpha=0.2, )\n",
    "    \n",
    "    \n",
    "    # Plot the mean to see similarity\n",
    "    ax.plot(diameters[1:-2]*10**9, df_cluster_mean.iloc[0,:].values, 'k:',alpha=0.3)\n",
    "    #ax.set_xticks(bin_cols[::5])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(0,1.1)\n",
    "    \n",
    "\n",
    "for spine in ['top', 'right']:\n",
    "    ax.spines[spine].set_visible(False)\n",
    "    ax.spines[spine].set_linewidth(0.5)\n",
    "    \n",
    "plt.xlabel('Dp [nm]')\n",
    "plt.title('K-means clustering (median+10/90th percentile)', loc='left')\n",
    "plt.ylabel('$dN/dLogD_p$ (normalized)')\n",
    "plt.legend(frameon=False,bbox_to_anchor=(.85, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cluster.iloc[0,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_norm_clustered_1h_90q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aaf55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec9787d",
   "metadata": {},
   "source": [
    "### Look at trends in 1h mean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e42a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create temporary copy\n",
    "df_tmp = df_hourly_2010_2020_mean.copy(deep = True)\n",
    "# df_tmp = df_tmp.drop(columns=['UFCPC','CPC3010','Ntot','flag'])\n",
    "# # =['UFCPC','5.012','630.957','707.946','CPC3010','Ntot','flag'])\n",
    "\n",
    "# In temporary copy of mean data add columns: \n",
    "\n",
    "\n",
    "# Create Nx/Ntot for 1 h mean data\n",
    "\n",
    "df_tmp = calcNtot(diameters[:len(bin_cols)+1], df_tmp)\n",
    "df_tmp_nxntot = df_tmp.copy(deep = True)\n",
    "\n",
    "df_tmp_nxntot['NxNtot'] = df_tmp_nxntot['NtotCalc']/df_tmp_nxntot['Ntot']\n",
    "\n",
    "# Drop NaN:s\n",
    "df_tmp_nxntot = df_tmp_nxntot.dropna(subset =['NxNtot'])\n",
    "\n",
    "# Look at annual cycle for NxNtot\n",
    "df_tmp_nxntot_mean = df_tmp_nxntot['NxNtot'].groupby(df_tmp_nxntot.index.month).mean()\n",
    "df_tmp_nxntot_std = df_tmp_nxntot['NxNtot'].groupby(df_tmp_nxntot.index.month).std()\n",
    "\n",
    "df_tmp_nxntot_median = df_tmp_nxntot['NxNtot'].groupby(df_tmp_nxntot.index.month).median()\n",
    "df_tmp_nxntot_10q = df_tmp_nxntot['NxNtot'].groupby(df_tmp_nxntot.index.month).quantile(0.1)\n",
    "df_tmp_nxntot_90q = df_tmp_nxntot['NxNtot'].groupby(df_tmp_nxntot.index.month).quantile(0.9)\n",
    "\n",
    "# Create UF CPC/CPC ratio for 1 h mean data\n",
    "df_tmp_rat =  df_tmp.copy(deep = True)\n",
    "df_tmp_rat['ratio_CPCs'] = df_tmp_rat['UFCPC']/df_tmp_rat['CPC3010']\n",
    "\n",
    "# Drop NaN:s\n",
    "df_tmp_rat = df_tmp_rat.dropna(subset =['ratio_CPCs'])\n",
    "\n",
    "df_1h_annual_cycle_rat_mean = df_tmp_rat['ratio_CPCs'].groupby(df_tmp_rat.index.month).mean()\n",
    "df_1h_annual_cycle_rat_std = df_tmp_rat['ratio_CPCs'].groupby(df_tmp_rat.index.month).std()\n",
    "8\n",
    "df_1h_annual_cycle_rat_median = df_tmp_rat['ratio_CPCs'].groupby(df_tmp_rat.index.month).median()\n",
    "df_1h_annual_cycle_rat_10q    = df_tmp_rat['ratio_CPCs'].groupby(df_tmp_rat.index.month).quantile(0.1)\n",
    "df_1h_annual_cycle_rat_90q    = df_tmp_rat['ratio_CPCs'].groupby(df_tmp_rat.index.month).quantile(0.9)\n",
    "\n",
    "# Create absolute diff Uf cpc - cpc----------------------------------------------------------\n",
    "\n",
    "df_tmp['abs_diff'] = np.absolute(df_tmp['UFCPC']-df_tmp['CPC3010'])\n",
    "\n",
    "df_tmp_adiff = df_tmp.copy(deep = True)\n",
    "\n",
    "# Drop NaN's\n",
    "df_tmp_adiff = df_tmp_adiff.dropna(subset =['abs_diff'])\n",
    "\n",
    "df_1h_annual_cycle_adiff_mean = df_tmp_adiff['abs_diff'].groupby(df_tmp_adiff.index.month).mean()\n",
    "df_1h_annual_cycle_adiff_std = df_tmp_adiff['abs_diff'].groupby(df_tmp_adiff.index.month).std()\n",
    "\n",
    "df_1h_annual_cycle_adiff_median = df_tmp_adiff['abs_diff'].groupby(df_tmp_adiff.index.month).median()\n",
    "df_1h_annual_cycle_adiff_10q = df_tmp_adiff['abs_diff'].groupby(df_tmp_adiff.index.month).quantile(0.1)\n",
    "df_1h_annual_cycle_adiff_90q = df_tmp_adiff['abs_diff'].groupby(df_tmp_adiff.index.month).quantile(0.9)\n",
    "\n",
    "# Create diff UFcpc and cpc----------------------------------------------------------\n",
    "\n",
    "df_tmp['diff_cpcs'] = df_tmp['UFCPC']-df_tmp['CPC3010']\n",
    "\n",
    "# Have to drop nans!!!!!!!\n",
    "\n",
    "df_tmp_diff = df_tmp.copy(deep = True)\n",
    "df_tmp_diff = df_tmp_diff.dropna(subset =['diff_cpcs'])\n",
    "\n",
    "df_1h_annual_cycle_diff_mean = df_tmp_diff['diff_cpcs'].groupby(df_tmp_diff.index.month).mean()\n",
    "df_1h_annual_cycle_diff_std = df_tmp_diff['diff_cpcs'].groupby(df_tmp_diff.index.month).std()\n",
    "\n",
    "df_1h_annual_cycle_diff_median = df_tmp_diff['diff_cpcs'].groupby(df_tmp_diff.index.month).median()\n",
    "df_1h_annual_cycle_diff_10q = df_tmp_diff['diff_cpcs'].groupby(df_tmp_diff.index.month).quantile(0.1)\n",
    "df_1h_annual_cycle_diff_90q = df_tmp_diff['diff_cpcs'].groupby(df_tmp_diff.index.month).quantile(0.9)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "\n",
    "def makeTrendPlotsMean(dfMean, dfStd, xL, yL, tL):\n",
    "    fig, axs = plt.subplots(1, figsize=(8, 5))\n",
    "    axs.plot(dfMean.index, \n",
    "             dfMean.values,\n",
    "             'o-', label= 'Mean')\n",
    "    min_std = dfMean.values - dfStd.values\n",
    "    #min_std[min_std<0]=0\n",
    "    \n",
    "    axs.fill_between(dfMean.index,\n",
    "                dfMean.values + dfStd.values,\n",
    "                min_std,\n",
    "                alpha=0.2,label= '+/-1$\\sigma$')\n",
    "    axs.legend(frameon=False)\n",
    "    axs.set_xlabel(xL)\n",
    "    axs.set_ylabel(yL)\n",
    "    axs.set_title(tL) \n",
    "    return fig, axs\n",
    "\n",
    "def makeTrendPlotsMedian(dfMedian, dfUpperQ, dfLowerQ, UQ, LQ, xL, yL, tL):\n",
    "    fig, axs = plt.subplots(1, figsize=(8, 5))\n",
    "    axs.plot(dfMedian.index, \n",
    "             dfMedian.values,\n",
    "             'ro-', label= 'Median')\n",
    "    axs.fill_between(dfMedian.index,\n",
    "                dfUpperQ,\n",
    "                dfLowerQ,\n",
    "                color ='r',alpha=0.2,label= str(LQ)+'-'+str(UQ)+' percentiles')\n",
    "    axs.legend(frameon=False)\n",
    "    axs.set_xlabel(xL)\n",
    "    axs.set_ylabel(yL)\n",
    "    axs.set_title(tL) \n",
    "    return fig, axs\n",
    "\n",
    "# Plot ratios from calculated area ratio\n",
    "fig1,axs1 = makeTrendPlotsMean(df_tmp_nxntot_mean,\n",
    "                             df_tmp_nxntot_std,\n",
    "                            'Month','$N_x/N_{tot}$',\n",
    "                            'Annual cycle (1 h mean) from calulated $N_x/N_{tot}$')\n",
    "\n",
    "fig12,axs12 = makeTrendPlotsMedian(df_tmp_nxntot_median,\n",
    "                            df_tmp_nxntot_90q,\n",
    "                            df_tmp_nxntot_10q,\n",
    "                            90,10,'Month','$N_x/N_{tot}$',\n",
    "                            'Annual cycle (1 h mean) $N_x/N_{tot}$')\n",
    "\n",
    "# Plot cpc ratios\n",
    "\n",
    "fig31,axs31 = makeTrendPlotsMean(df_1h_annual_cycle_rat_mean,\n",
    "                             df_1h_annual_cycle_rat_std,\n",
    "                            'Month','$CPC_{UF}/CPC$',\n",
    "                            'Annual cycle (1 h mean) $CPC_{UF}/CPC$')\n",
    "\n",
    "fig32,axs32 = makeTrendPlotsMedian(df_1h_annual_cycle_rat_median,\n",
    "                                    df_1h_annual_cycle_rat_90q,\n",
    "                                    df_1h_annual_cycle_rat_10q,\n",
    "                                    90,10,\n",
    "                                    'Month','$CPC_{UF}/CPC$',\n",
    "                                    'Annual cycle (1 h mean) $CPC_{UF}/CPC$')\n",
    "\n",
    "# Plot abs diff cpc:s\n",
    "\n",
    "fig41,axs41 = makeTrendPlotsMean(df_1h_annual_cycle_adiff_mean,\n",
    "                             df_1h_annual_cycle_adiff_std,\n",
    "                            'Month','$|CPC_{UF}-CPC|$',\n",
    "                            'Annual cycle (1 h mean) $|CPC_{UF}-CPC$|')\n",
    "\n",
    "fig42,axs42 = makeTrendPlotsMedian(df_1h_annual_cycle_adiff_median,\n",
    "                                    df_1h_annual_cycle_adiff_90q,\n",
    "                                    df_1h_annual_cycle_adiff_10q,\n",
    "                                    90,10,\n",
    "                                    'Month','$|CPC_{UF}-CPC|$',\n",
    "                                    'Annual cycle (1 h mean) $|CPC_{UF}-CPC$|')\n",
    "axs42.plot()\n",
    "\n",
    "# Plot diff cpc:s\n",
    "fig52,axs52 = makeTrendPlotsMean(df_1h_annual_cycle_diff_mean,\n",
    "                                     df_1h_annual_cycle_diff_std,\n",
    "                                    'Month','$CPC_{UF}-CPC$',\n",
    "                                    'Annual cycle (1 h mean) $CPC_{UF}-CPC$')\n",
    "\n",
    "fig61,axs61 = makeTrendPlotsMedian(df_1h_annual_cycle_diff_median,\n",
    "                                    df_1h_annual_cycle_diff_90q,\n",
    "                                    df_1h_annual_cycle_diff_10q,\n",
    "                                    90,10,\n",
    "                                    'Month','$CPC_{UF}-CPC$',\n",
    "                                    'Annual cycle (1 h mean) $CPC_{UF}-CPC$')\n",
    "\n",
    "fig62,axs62 = makeTrendPlotsMean(df_1h_annual_cycle_diff_mean,\n",
    "                                     df_1h_annual_cycle_diff_std,\n",
    "                                    'Month','$CPC_{UF}-CPC$',\n",
    "                                    'Annual cycle (1 h mean) $CPC_{UF}-CPC$')\n",
    "# print(df_hourly_2010_2020_mean_copy.shape)\n",
    "\n",
    "# # Create abs(UF_CPC-CPC ) for 1 h mean data \n",
    "# df_hourly_2010_2020_mean_copy['absDiff'] = np.absolute( df_tmp['UFCPC']-df_tmp['CPC3010'])\n",
    "# print(df_hourly_2010_2020_mean_copy.shape)\n",
    "\n",
    "\n",
    "df_cluster = df_tmp_adiff.copy(deep = 'true')\n",
    "# Create a column called day of year (DoY) which gives the day of the year 1 to 366\n",
    "df_cluster.loc[:,'DoY'] = df_cluster.index.dayofyear\n",
    "df_cluster = df_cluster.dropna(subset =['DoY'])\n",
    "\n",
    "# Calculate the occurence of cluster \"cluster\" per month\n",
    "df_cluster_count = df_cluster.groupby('DoY').count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1345618f",
   "metadata": {},
   "source": [
    "# For presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f4ebc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def makeTrendPlotsMedian2(dfMedian, dfUpperQ, dfLowerQ, UQ, LQ, xL, yL, tL):\n",
    "    fig, axs = plt.subplots(1, figsize=(8, 5))\n",
    "    axs.plot(dfMedian.index, \n",
    "             dfMedian.values,\n",
    "             'ro-', label= 'Median $|CPC_{UF}-CPC$|')\n",
    "    axs.fill_between(dfMedian.index,\n",
    "                dfUpperQ,\n",
    "                dfLowerQ,\n",
    "                color ='r',alpha=0.2,label= str(LQ)+'-'+str(UQ)+' percentiles')\n",
    "    axs.legend(frameon=False)\n",
    "    axs.set_xlabel(xL)\n",
    "    axs.set_ylabel(yL)\n",
    "    axs.set_title(tL) \n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "\n",
    "# Plot abs diff cpc:s\n",
    "\n",
    "\n",
    "fig66,axs66 = makeTrendPlotsMedian2(df_1h_annual_cycle_adiff_median,\n",
    "                                    df_1h_annual_cycle_adiff_90q,\n",
    "                                    df_1h_annual_cycle_adiff_10q,\n",
    "                                    90,10,\n",
    "                                    'Month','$|CPC_{UF}-CPC|$',\n",
    "                                    '$|CPC_{UF}-CPC$|')\n",
    "axs66.plot(df_1h_annual_cycle_adiff_mean.index,df_1h_annual_cycle_adiff_mean,'r',\n",
    "          label = 'Mean $|CPC_{UF}-CPC$|', ls=':')\n",
    "#axs66.legend(frameon=False) \n",
    "\n",
    "\n",
    "# Create a temporary copy of cluster-assigned data (normalized)\n",
    "df_norm_clustered_1h_mean_copy = df_norm_clustered_1h_mean.copy()\n",
    "\n",
    "clusters = [1, 2]\n",
    "\n",
    "\n",
    "for cluster in clusters:\n",
    "    df_cluster = df_norm_clustered_1h_mean_copy[df_norm_clustered_1h_mean_copy['clusters'] == cluster]\n",
    "    df_cluster = df_cluster.copy()\n",
    "    \n",
    "    # Create a cloumn called month in dataframe\n",
    "    df_cluster.loc[:,'month'] =  df_cluster.index.month\n",
    "    \n",
    "    # Calculate the occurence of cluster \"cluster\" per month\n",
    "    df_cluster_count = df_cluster.groupby('month').count()\n",
    "    \n",
    "    axs66.plot(df_cluster_count.index, \n",
    "               df_cluster_count.iloc[:,0], 'o-', \n",
    "               label='cluster: '+str(cluster),\n",
    "              alpha = 0.4, ls= ':')\n",
    "    \n",
    "    axs66.set_xlabel('Month')\n",
    "    axs66.set_ylabel('Occurence')\n",
    "    axs66.legend(frameon=False)\n",
    "plt.legend(frameon=False,bbox_to_anchor=(1, 1))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042df3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary copy of cluster-assigned data (normalized)\n",
    "df_norm_clustered_1h_mean_copy = df_norm_clustered_1h_mean.copy()\n",
    "\n",
    "clusters = [1, 2, 3, 4, 5]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "for cluster in clusters:\n",
    "    df_cluster = df_norm_clustered_1h_mean_copy[df_norm_clustered_1h_mean_copy['clusters'] == cluster]\n",
    "    df_cluster = df_cluster.copy()\n",
    "    \n",
    "    # Create a cloumn called month in dataframe\n",
    "    df_cluster.loc[:,'month'] =  df_cluster.index.month\n",
    "    \n",
    "    # Calculate the occurence of cluster \"cluster\" per month\n",
    "    df_cluster_count = df_cluster.groupby('month').count()\n",
    "    \n",
    "    ax.plot(df_cluster_count.index, df_cluster_count.iloc[:,0], 'o-', label='cluster: '+str(cluster))\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Occurence')\n",
    "    ax.legend(frameon=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# Make stack plot from montly resample\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "for cluster in clusters:\n",
    "    df_cluster = df_norm_clustered_1h_mean_copy[df_norm_clustered_1h_mean_copy['clusters'] == cluster]\n",
    "    df_cluster = df_cluster.copy()\n",
    "    \n",
    "    # Create a cloumn called month in dataframe\n",
    "    df_cluster.loc[:,'month'] =  df_cluster.index.month\n",
    "    \n",
    "    # Calculate the occurence of cluster \"cluster\" per month\n",
    "    df_cluster_count = df_cluster.groupby('month').count()\n",
    "    \n",
    "    ax.plot(df_cluster_count.index, df_cluster_count.iloc[:,0], 'o-', label='cluster: '+str(cluster))\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Occurence')\n",
    "    ax.legend(frameon=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for cluster in clusters:\n",
    "#     df_cluster = df_norm_clustered_1h_mean_copy[df_norm_clustered_1h_mean_copy['clusters'] == cluster]\n",
    "#     df_cluster = df_cluster.copy()\n",
    "    \n",
    "#     # Create a column called day of year (DoY) which gives the day of the year 1 to 366\n",
    "#     df_cluster.loc[:,'DoY'] = df_cluster.index.dayofyear\n",
    "    \n",
    "#     # Calculate the occurence of cluster \"cluster\" per month\n",
    "#     df_cluster_count = df_cluster.groupby('DoY').count()\n",
    "    \n",
    "#     # Plot the occurence of cluster X vs the day of year\n",
    "#     ax.plot(df_cluster_count.index, df_cluster_count.iloc[:,0], '-', label='cluster: '+str(cluster))\n",
    "#     ax.set_xlabel('Day of Year')\n",
    "#     ax.set_ylabel('Frequency')\n",
    "#     ax.legend(frameon=False) \n",
    "    \n",
    "# fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "\n",
    "# # Normalized\n",
    "\n",
    "# for cluster in clusters:\n",
    "#     df_cluster = df_norm_clustered_1h_mean_copy[df_norm_clustered_1h_mean_copy['clusters'] == cluster]\n",
    "#     df_cluster = df_cluster.copy()\n",
    "    \n",
    "#     # Create a column called day of year (DoY) which gives the day of the year 1 to 366\n",
    "#     df_cluster.loc[:,'DoY'] = df_cluster.index.dayofyear\n",
    "    \n",
    "#     # Calculate the occurence of cluster \"cluster\" per month\n",
    "#     df_cluster_count = df_cluster.groupby('DoY').count()\n",
    "    \n",
    "#     # Plot the occurence of cluster X vs the day of year\n",
    "#     ax.plot(df_cluster_count.index,\n",
    "#             df_cluster_count.iloc[:,0]/df_cluster_count.iloc[:,0].max(),\n",
    "#             '-', label='cluster: '+str(cluster))\n",
    "    \n",
    "#     ax.set_xlabel('Day of Year')\n",
    "#     ax.set_ylabel('Normalized Frequency')\n",
    "#     ax.legend(frameon=False) \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5ce0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters_month = pd.DataFrame(columns=clusters)\n",
    "print(df_clusters_month)\n",
    "for cluster in clusters:\n",
    "    df_cluster = df_norm_clustered_1h_mean_copy[df_norm_clustered_1h_mean_copy['clusters'] == cluster]\n",
    "    df_cluster = df_cluster.copy()    \n",
    "    df_cluster.loc[:,'month'] =  df_cluster.index.month    \n",
    "    # Calculate the occurence of cluster \"cluster\" per month\n",
    "    df_cluster_count = df_cluster.groupby('month').count()    \n",
    "    monthly_occurance = df_cluster_count.iloc[:,0].values\n",
    "    print(monthly_occurance)\n",
    "    df_clusters_month[cluster] = monthly_occurance\n",
    "    \n",
    "df_clusters_month['total_freq'] = df_clusters_month.sum(axis=1)\n",
    "df_clusters_month = df_clusters_month.div(df_clusters_month['total_freq'], axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691fb126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters_month[clusters].plot(kind='bar', \n",
    "                    stacked=True, \n",
    "                    colormap='Set2',\n",
    "                    width = 0.9,            \n",
    "                    figsize=(10, 6))\n",
    "\n",
    "\n",
    "\n",
    "plt.ylabel(\"Normalized occurence\")\n",
    "plt.ylim(0,1)\n",
    "plt.legend(title = 'Cluster:',frameon=False,bbox_to_anchor=(1, 1))\n",
    "plt.xticks(np.arange(0, 12, 1), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec'],)\n",
    "plt.xticks(rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.loc[:,'date'] = df_cluster.index.map(datetime.datetime.toordinal) \n",
    "df_cluster.loc[:,'date'] = df_cluster.loc[:,'date'] - df_cluster.loc[:,'date'][0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb65d0",
   "metadata": {},
   "source": [
    "### Make nice MJJ trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trend = df_norm_clustered_1h_mean_copy.copy()\n",
    "\n",
    "# df_trend['Month'] = df_trend.index\n",
    "# df_trend['Year']  =\n",
    "dt_array = df_trend.index.values\n",
    "df_trend['dtObjects'] = dt_array\n",
    "df_trend['month_year'] = pd.to_datetime(df_trend['dtObjects']).dt.to_period('M')\n",
    "df_trend['month'] = df_trend['dtObjects'].dt.month\n",
    "df_trend['year'] = df_trend['dtObjects'].dt.year\n",
    "\n",
    "clusters = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Create a dataframe from dictionaries as sometimes ther might not be any cluster 3 for example in some month --> \n",
    "# Problem that when we group by month-year we don't get the \n",
    "\n",
    "list_of_dicts = []\n",
    "for cluster in clusters:\n",
    "    df_cluster = df_trend[df_trend['clusters'] == cluster]\n",
    "    df_cluster = df_cluster.copy()    \n",
    "    #print(cluster)   \n",
    "    # Calculate the occurence of cluster \"cluster\" per month\n",
    "    df_cluster_count  = df_cluster.groupby('month_year').count()    \n",
    "    monthly_occurance = df_cluster_count.iloc[:,0].values\n",
    "    \n",
    "    \n",
    "    dict_cluster = dict(zip(df_cluster_count.index, monthly_occurance))\n",
    "    \n",
    "    #print(dict_cluster)\n",
    "    list_of_dicts.append(dict_cluster)\n",
    "    #print(list_of_dicts)\n",
    "\n",
    "# ds is a dataframe whcih contains the rows = cluster no, and rows equal to month-year. \n",
    "df_clusters_seqMonth = pd.DataFrame(list_of_dicts)\n",
    "#     print(monthly_occurance)\n",
    "#     df_clusters_seqmonth[cluster] = monthly_occurance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_trend.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d46ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The days that have zero count get a Nan values that should be replaced by 0\n",
    "df_clusters_seqMonth = df_clusters_seqMonth.replace(np.nan, 0)\n",
    "\n",
    "df_clusters_seqMonth.loc['total'] = df_clusters_seqMonth.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeeb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters_seqMonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(df_clusters_seqMonth.columns.values[0])\n",
    "datetimes = [pd.to_datetime(str(x)) for x in list(df_clusters_seqMonth.columns.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data frame so clusters are columns \n",
    "df_T = df_clusters_seqMonth.T\n",
    "df_T.index = datetimes\n",
    "df_T['month'] = df_T.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a92cc53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c4942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T.columns = ['1','2','3','4','5','total', 'month']\n",
    "df_T_summer = df_T[df_T.month.isin([5,6,7])] # Choosing the months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T_summer = df_T_summer.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_summer = df_T_summer[['1', '2', '3', '4', '5']].div(df_T_summer['total'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_summer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabecddf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Trend for MJJ\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.plot(df_norm_summer.index,df_norm_summer['1'].values,'o' )\n",
    "ax.set_ylabel('Normalized frequency')\n",
    "ax.set_xlabel('Datetime')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "#-----------Choosing the clusters---------------------------\n",
    "\n",
    "cluster_name = '2'\n",
    "\n",
    "\n",
    "ax.plot(df_norm_summer.index,\n",
    "        df_norm_summer[cluster_name].values,\n",
    "        'o',label='original data') \n",
    "ax.set_ylabel('Normalized frequency')\n",
    "ax.set_xlabel('Datetime')\n",
    "plt.show()\n",
    "\n",
    "# Fit a lin regression slope is not what we want -we don't want correlation -we want trend\n",
    "varx = np.arange(len(df_norm_summer.index))\n",
    "vary = df_norm_summer[cluster_name].values\n",
    "\n",
    "# # # mask = ~np.isnan(varx) & ~np.isnan(vary)\n",
    "# res = sc.stats.linregress(varx, vary)\n",
    "\n",
    "# print(f\"R-squared: {res.rvalue**2:.6f}\")\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,5))\n",
    "# plt.plot(varx,\n",
    "#          vary,\n",
    "#          'o', label='original data')\n",
    "# plt.plot(varx,\n",
    "#          res.intercept + res.slope*varx,\n",
    "#          'r-', label='fitted line')\n",
    "# plt.legend()\n",
    "# plt.show()    \n",
    "# plt.ylabel('N_tot')\n",
    "# plt.xlabel('N_tot_calc')\n",
    "\n",
    "\n",
    "# For trend analysis we use theil zen slope\n",
    "slope, intercept, lo_slope, up_slope  = sc.stats.theilslopes(vary, varx, 0.95)\n",
    "\n",
    "# medslope :Theil slope.\n",
    "# medintercept : Intercept of the Theil line, as median(y) - medslope*median(x).\n",
    "# lo_slope : Lower bound of the confidence interval on medslope.\n",
    "# up_slope : Upper bound of the confidence interval on medslope.\n",
    "\n",
    "print('---------------------------')\n",
    "print('Theil-Sen slope')\n",
    "print('Slope:',slope)\n",
    "print('Intercept:',intercept)\n",
    "print('Lower bound of the confidence interval on medslopeLower:',lo_slope)\n",
    "print('Upper bound of the confidence interval on medslopeLower:',up_slope)\n",
    "print('---------------------------')\n",
    "\n",
    "# Plot the Theil-Sen slope (non-parametric)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(varx, vary, 'b.',label='original data')\n",
    "ax.plot(varx, intercept + slope * varx, 'b-')\n",
    "\n",
    "ax.fill_between(varx, \n",
    "               intercept + up_slope * varx,\n",
    "               intercept + lo_slope * varx,\n",
    "               alpha=0.2, label= '95% Confidence $')\n",
    "plt.show()\n",
    "ax.legend()\n",
    "\n",
    "# Test the sign of the slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d863b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_summer['year'] = df_norm_summer.index.year\n",
    "df_norm_summer['year'] = df_norm_summer['year'] - df_norm_summer['year'].iloc[0]\n",
    "df_norm_summer['month'] = df_norm_summer.index.month\n",
    "df_norm_summer['month_ordered'] = df_norm_summer['year']*12 + df_norm_summer['month'] \n",
    "df_norm_summer['month_ordered'] = df_norm_summer['month_ordered'] - df_norm_summer['month_ordered'].iloc[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7246bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm_summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2959473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice\n",
    "df_norm_summer = df_norm_summer[df_norm_summer.index.year < 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = '2'\n",
    "\n",
    "# Fit a lin regression slope is not what we want -we don't want correlation -we want trend\n",
    "varx = df_norm_summer['month_ordered'].values\n",
    "vary = df_norm_summer[cluster_name].values\n",
    "\n",
    "# For trend analysis we use theil zen slope\n",
    "slope, intercept, lo_slope, up_slope  = sc.stats.theilslopes(vary, varx, 0.95)\n",
    "\n",
    "print('---------------------------')\n",
    "print('Theil-Sen slope')\n",
    "print('Slope:',slope)\n",
    "print('Intercept:',intercept)\n",
    "print('Lower bound of the confidence interval on medslopeLower:',lo_slope)\n",
    "print('Upper bound of the confidence interval on medslopeLower:',up_slope)\n",
    "print('---------------------------')\n",
    "\n",
    "plt.rcParams['font.size'] = '15'\n",
    "# Plot the Theil-Sen slope (non-parametric)\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_norm_summer.index, df_norm_summer[cluster_name].values, 'x-', ls=':')\n",
    "ax.set_ylabel('NPF events (normalized hours)')\n",
    "\n",
    "ax2 = ax.twiny()\n",
    "ax2.plot(varx, intercept + slope * varx, 'b-', ls='-', alpha=1, \n",
    "         label='y = '+str( round(slope, 3) )+'x +' +str(round(intercept,3) ))\n",
    "ax2.fill_between(varx, \n",
    "               intercept + up_slope * varx,\n",
    "               intercept + lo_slope * varx,\n",
    "               alpha=0.2, label= '95% Confidence')\n",
    "#ax.set_ylim(0,1)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_title('Cluster 2: Trend MJJ')\n",
    "\n",
    "for ax in [ax, ax2]:\n",
    "    ax.spines.right.set_visible(False)\n",
    "    ax.spines.top.set_visible(False)\n",
    "    ax.legend(frameon=False)\n",
    "\n",
    "# no = 10\n",
    "# t = pd.date_range(start='2011-05-01',\n",
    "#                   end='2021-05-01',\n",
    "#                   periods=11)\n",
    "# x_val_months = range(1,122,11)\n",
    "\n",
    "#ax.set_xticks(df_norm_summer['month_ordered'].values[::6],  df_norm_summer.index[::6])\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef902fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = '1'\n",
    "\n",
    "# Fit a lin regression slope is not what we want -we don't want correlation -we want trend\n",
    "varx = df_norm_summer['month_ordered'].values\n",
    "vary = df_norm_summer[cluster_name].values\n",
    "\n",
    "# For trend analysis we use theil zen slope\n",
    "slope, intercept, lo_slope, up_slope  = sc.stats.theilslopes(vary, varx, 0.95)\n",
    "\n",
    "print('---------------------------')\n",
    "print('Theil-Sen slope')\n",
    "print('Slope:',slope)\n",
    "print('Intercept:',intercept)\n",
    "print('Lower bound of the confidence interval on medslopeLower:',lo_slope)\n",
    "print('Upper bound of the confidence interval on medslopeLower:',up_slope)\n",
    "print('---------------------------')\n",
    "\n",
    "# Plot the Theil-Sen slope (non-parametric)\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(df_norm_summer.index, df_norm_summer[cluster_name].values, 'rx-', ls=':')\n",
    "ax.set_ylabel('NPF events (normalized hours)')\n",
    "\n",
    "ax2 = ax.twiny()\n",
    "ax2.plot(varx, intercept + slope * varx, 'r-', ls='-', alpha=1, \n",
    "         label='y = '+str( round(slope, 3) )+'x +' +str(round(intercept,3) ))\n",
    "ax2.fill_between(varx, \n",
    "               intercept + up_slope * varx,\n",
    "               intercept + lo_slope * varx,\n",
    "               alpha=0.2, label= '95% Confidence',\n",
    "                facecolor = 'r')\n",
    "#ax.set_ylim(0,1)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_title('Cluster 1: Trend MJJ')\n",
    "\n",
    "for ax in [ax, ax2]:\n",
    "    ax.spines.right.set_visible(False)\n",
    "    ax.spines.top.set_visible(False)\n",
    "    ax.legend(frameon=False)\n",
    "\n",
    "# no = 10\n",
    "# t = pd.date_range(start='2011-05-01',\n",
    "#                   end='2021-05-01',\n",
    "#                   periods=11)\n",
    "# x_val_months = range(1,122,11)\n",
    "\n",
    "#ax.set_xticks(df_norm_summer['month_ordered'].values[::6],  df_norm_summer.index[::6])\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0913c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
